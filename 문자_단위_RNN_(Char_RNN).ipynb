{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… ë¬¸ì ë‹¨ìœ„ RNN (Char RNN)\n",
        "- ì…ì¶œë ¥ì˜ ë‹¨ìœ„ê°€ ë‹¨ì–´ ë ˆë²¨ì´ ì•„ë‹ˆë¼ ë¬¸ì ë ˆë²¨"
      ],
      "metadata": {
        "id": "nh7nG6JA5p4Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF3ngpIX5l4J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… 1. í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬\n",
        "- ì…ë ¥ ë°ì´í„°ì™€ ë ˆì´ë¸”ì— ëŒ€í•´ì„œ ë¬¸ì ì§‘í•©ì„ ë§Œë“¤ê² ìŠµë‹ˆë‹¤. -> ì¤‘ë³µì€ ì œì™¸í•´ì•¼ í•¨!"
      ],
      "metadata": {
        "id": "FMYn0Zdm6CHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = 'apple'\n",
        "output_str = 'pple!'\n",
        "\n",
        "char_vocab = sorted(list(set((input_str + output_str))))\n",
        "\n",
        "vocab_size = len(char_vocab)\n",
        "print('ë¬¸ì ì§‘í•© í¬ê¸° : {}'.format(vocab_size))"
      ],
      "metadata": {
        "id": "wzpIgYJX6JHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ“Œ ì…ë ¥ì€ one-hot ë²¡í„°ë¥¼ ì‚¬ìš©í•  ê²ƒì´ê¸° ë•Œë¬¸ì— **ì…ë ¥ì˜ í¬ê¸°ëŠ” ë¬¸ì ì§‘í•©ì˜ í¬ê¸°**ê°€ ë˜ì–´ì•¼ í•¨"
      ],
      "metadata": {
        "id": "uQu_BVi86JuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = vocab_size\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "lr = 0.1"
      ],
      "metadata": {
        "id": "B2rGarlG6Jr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ“Œ ë¬¸ì ì§‘í•©ì— ë ˆì´ë¸” ë¶€ì—¬"
      ],
      "metadata": {
        "id": "9D8lZxNa7NPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab))\n",
        "\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "id": "H4lt2uS86Jpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char = dict()\n",
        "\n",
        "for k, v in char_to_index.items():\n",
        "    index_to_char[v] = k\n",
        "\n",
        "print(index_to_char)"
      ],
      "metadata": {
        "id": "wYen4UyA6JnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ“Œ ì…ë ¥ ë°ì´í„°ì™€ ë ˆì´ë¸” ë°ì´í„°ë¥¼ êµ¬ì„±í•˜ëŠ” ë¬¸ìë“¤ì„ ì •ìˆ˜ë¡œ ë§µí•‘í•˜ê² ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "hn4Snj2U8Tqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in output_str]\n",
        "\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "id": "XH4bVD7J6JkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ“Œ NLPì—ì„œëŠ” 3ì°¨ì› í…ì„œê°€ ê¸°ë³¸ì´ê¸° ë•Œë¬¸ì—, ë°°ì¹˜ ì°¨ì›ì„ ì¶”ê°€í•´ì£¼ê² ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "cdwT-4Oa9gar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°°ì¹˜ ì°¨ì› ì¶”ê°€\n",
        "\n",
        "x_data_ = [x_data]\n",
        "y_data_ = [y_data]\n",
        "\n",
        "print(x_data_)\n",
        "print(y_data_)"
      ],
      "metadata": {
        "id": "4SNZ4Ua46JgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytorchì—ì„œì˜ í…ì„œ ì—°ì‚°ì¸ unsqueeze(0)ë„ í™œìš© ê°€ëŠ¥\n",
        "\n",
        "x_data2 = torch.tensor(x_data).unsqueeze(0)\n",
        "y_data2 = torch.tensor(y_data).unsqueeze(0)\n",
        "\n",
        "print(x_data2.shape)\n",
        "print(y_data2.shape)"
      ],
      "metadata": {
        "id": "Z0LkHR-o6Jdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ“Œ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê° ë¬¸ìë“¤ì„ one-hot ë²¡í„°ë¡œ ë³€í™˜!"
      ],
      "metadata": {
        "id": "-CtYa-hw-KWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data_]\n",
        "\n",
        "print(x_one_hot)"
      ],
      "metadata": {
        "id": "Wjsj93xi6Jav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ“Œ ì…ë ¥ê³¼ ë ˆì´ë¸” ë°ì´í„°ë¥¼ í…ì„œë¡œ ìºìŠ¤íŒ… í•´ì£¼ê² ìŠµë‹ˆë‹¤"
      ],
      "metadata": {
        "id": "tPu-4hOY-x-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data_)"
      ],
      "metadata": {
        "id": "V-Yk9XB9-HRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í…ì„œì˜ shape í™•ì¸\n",
        "\n",
        "print('í•™ìŠµ ë°ì´í„°ì˜ í¬ê¸° : {}'.format(X.shape))\n",
        "print('ë ˆì´ë¸” ë°ì´í„°ì˜ í¬ê¸° : {}'.format(Y.shape))"
      ],
      "metadata": {
        "id": "jwFM11SO-HO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… ëª¨ë¸ êµ¬í˜„í•˜ê¸°"
      ],
      "metadata": {
        "id": "xFk1t82h_fP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size, bias=True) # ì¶œë ¥ì¸µ\n",
        "\n",
        "    def forward(self, x): # êµ¬í˜„í•œ RNN ì…€ê³¼ FC ì¸µì„ ì—°ê²°í•˜ëŠ” ì—­í• \n",
        "\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "4QMWBqyg-HL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "om0ywPEq-HHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "id": "3EVY3MLQ-HFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ“Œ (1, 5, 5)ì˜ í¬ê¸°ë¥¼ ê°€ì§€ëŠ” output tensorë¥¼ ë‚˜ì¤‘ì— ì •í™•ë„ ì¸¡ì •ì„ ìœ„í•´ 2ì°¨ì› í…ì„œë¡œ ë°”ê¾¸ëŠ” ê³¼ì •ì´ í•„ìš”í•¨"
      ],
      "metadata": {
        "id": "J9htGQbOESIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, output_size).shape)"
      ],
      "metadata": {
        "id": "Tc5I_viB-HCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "id": "1T7Y2PpxAx3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ“Œ Optimzerì™€ loss function(criterion)ì„ ì •ì˜í•´ ë´…ì‹œë‹¤"
      ],
      "metadata": {
        "id": "jUiWeMdQFEUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr)"
      ],
      "metadata": {
        "id": "06fWtjxoAx0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training code\n",
        "\n",
        "epoch = 100\n",
        "\n",
        "for i in range(epoch):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(X)\n",
        "\n",
        "    loss = criterion(outputs.view(-1, output_size), Y.view(-1)) # view() -> Batch dimension ì œê±°ë¥¼ ìœ„í•´\n",
        "    loss.backward() # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    # í”„ë¡œê·¸ë˜ìŠ¤ ë°”\n",
        "    result = outputs.data.numpy().argmax(axis=2) # output(5ê°œì˜ ê°’)ì— ëŒ€í•´ì„œ ê°€ì¥ ë†’ì€ ê°’ì˜ ì¸ë±ìŠ¤ ì„ íƒ!, ìˆ«ì í˜•íƒœ\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)]) # helper functionì„ í˜¸ì¶œí•˜ì—¬ charë¥¼ str í˜•íƒœë¡œ ë³€í™˜\n",
        "    print(i, \"loss : \", loss.item(), \"prediction : \", result, \"label : \", y_data, \"output_str : \", result_str)"
      ],
      "metadata": {
        "id": "MF8TKBBxAxuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = 'ability'\n",
        "output_str = 'ity!!!!'\n",
        "\n",
        "char_vocab = sorted(list(set((input_str + output_str))))\n",
        "\n",
        "vocab_size = len(char_vocab)\n",
        "\n",
        "input_size = vocab_size\n",
        "hidden_size = 7\n",
        "output_size = 7\n",
        "lr = 0.1\n",
        "\n",
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab))\n",
        "\n",
        "index_to_char = dict()\n",
        "\n",
        "for k, v in char_to_index.items():\n",
        "    index_to_char[v] = k\n",
        "\n",
        "print(index_to_char)\n",
        "\n",
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in output_str]\n",
        "\n",
        "x_data_ = [x_data]\n",
        "y_data_ = [y_data]\n",
        "\n",
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data_]\n",
        "\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data_)\n",
        "\n",
        "net = Net(input_size, hidden_size, output_size)\n",
        "\n",
        "outputs = net(X)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr)\n",
        "\n",
        "epoch = 100\n",
        "\n",
        "for i in range(epoch):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(X)\n",
        "\n",
        "    loss = criterion(outputs.view(-1, output_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    result = outputs.data.numpy().argmax(axis=2)\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss : \", loss.item(), \"prediction : \", result, \"label : \", y_data, \"output_str : \", result_str)"
      ],
      "metadata": {
        "id": "FqUEGskuAxrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“ ë¬¸ì ë‹¨ìœ„ RNN - with Sentence data"
      ],
      "metadata": {
        "id": "xWqhuAknK27D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "csAUTEEPAxmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… 1. í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°"
      ],
      "metadata": {
        "id": "kmMpAY0OLCh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ],
      "metadata": {
        "id": "3Jfh1rPjAxiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sentence))"
      ],
      "metadata": {
        "id": "A4qQyjeXLmMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = sorted(list(set(sentence))) # ë¬¸ì ì§‘í•© ìƒì„±\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # ê° ë¬¸ìì— ì •ìˆ˜ ì¸ì½”ë”©\n",
        "\n",
        "print(char_dic) # â—â— ë¬¸ìì—ì„œëŠ” ê³µë°±ë„ í•˜ë‚˜ì˜ ì›ì†Œë¡œ ëŒ€ì‘ â—â—"
      ],
      "metadata": {
        "id": "lpM3oFV3LmKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ“Œ ê° ë¬¸ìì— single-digit ë ˆì´ë¸”ì„ ë¶€ì—¬í•˜ì˜€ìœ¼ë©°, 25ì˜ í¬ê¸°ë¥¼ ê°€ì§€ëŠ” ë¬¸ì ì§‘í•© ì™„ì„±"
      ],
      "metadata": {
        "id": "gzBcAIelMIN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('ë¬¸ì ì§‘í•©ì˜ í¬ê¸° : {}'.format(len(char_dic)))"
      ],
      "metadata": {
        "id": "OUyf_Wd8LmIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ“Œ ì…ë ¥ì€ one-hot ë²¡í„°ë¡œ ì‚¬ìš©í•  ê²ƒì´ê¸° ë•Œë¬¸ì— ë³€í™˜ ê³¼ì • í•„ìš”"
      ],
      "metadata": {
        "id": "UdM_MO_dMWjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¬¸ì¥ ë°ì´í„°ì´ê¸° ë•Œë¬¸ì— ë‹¨ìœ„ë³„ë¡œ ëŠì–´ì„œ ì²˜ë¦¬\n",
        "# sentence_lengthë¼ëŠ” ë³€ìˆ˜ë¥¼ í™œìš©í•´ë³´ì\n",
        "\n",
        "hidden_size = len(char_dic)\n",
        "sentence_length = 10 # ë¬¸ì¥ì„ ë¬¸ì 10ê°œ ë‹¨ìœ„ë¡œ ëŠì–´ì„œ ì „ì²˜ë¦¬\n",
        "lr = 0.1"
      ],
      "metadata": {
        "id": "YHZl0yDhLmGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° êµ¬ì„±\n",
        "\n",
        "x_data = list()\n",
        "y_data = list()\n",
        "\n",
        "for i in range(0, len(sentence) - sentence_length):\n",
        "    x_str = sentence[i: i + sentence_length]\n",
        "    y_str = sentence[i + 1: i + sentence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str]) # ì…ë ¥ ë°ì´í„°ë¥¼ single-digití™”\n",
        "    y_data.append([char_dic[c] for c in y_str])"
      ],
      "metadata": {
        "id": "KvSX1q0GLmDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ“Œ í˜„ì¬ê¹Œì§€ ì´ 170ê°œë¡œ êµ¬ì„±ëœ ë°ì´í„° ì…‹(x, y í˜ì–´)ë¥¼ ìƒì„±í•˜ì˜€ìŒ. ê° í˜ì–´ì˜ ì…ë ¥ ë°ì´í„°ëŠ” ê³ ìœ  ì •ìˆ˜ë¡œ ì¸ì½”ë”©ì´ ëœ ìƒíƒœ"
      ],
      "metadata": {
        "id": "gaN9i2ucOL7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "id": "9aDvnDLPLmBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(len(char_dic))[x] for x in x_data] # one-hot ë²¡í„° ìƒˆì• ì„±\n",
        "\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "4akiJydULl_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('í•™ìŠµ ë°ì´í„°ì˜ í¬ê¸° : {}'.format(X.shape))\n",
        "print('ë ˆì´ë¸” ë°ì´í„°ì˜ í¬ê¸° : {}'.format(Y.shape))"
      ],
      "metadata": {
        "id": "J2GZipT7AxeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y[0])"
      ],
      "metadata": {
        "id": "_Ry0w4hTPZad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… ëª¨ë¸ êµ¬í˜„í•˜ê¸°"
      ],
      "metadata": {
        "id": "-0hkUcnZPiWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, layers):\n",
        "\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "U2VvygADPZW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_size = len(char_dic)\n",
        "\n",
        "net = Net(dic_size, hidden_size, 2) # hidden layerê°€ 2ì¸µ ì§œë¦¬ì¸ DRNN"
      ],
      "metadata": {
        "id": "Z9D5EM4ePZRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì†ì‹¤ í•¨ìˆ˜\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì €\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr)"
      ],
      "metadata": {
        "id": "oJFcFhfUPZPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… í•™ìŠµ ì§„í–‰"
      ],
      "metadata": {
        "id": "s65AFPYeTecV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 100\n",
        "\n",
        "for i in range(epoch):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(X) # (170, 10, 25) í¬ê¸°ë¥¼ ê°€ì§„ í…ì„œë¥¼ ë§¤ ì—í¬í¬ë§ˆë‹¤ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©\n",
        "\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    # resultsì˜ í…ì„œ í¬ê¸°ëŠ” (170, 10)\n",
        "    results = outputs.argmax(dim=2)\n",
        "\n",
        "    predict_str = str()\n",
        "\n",
        "    for j, result in enumerate(results) :\n",
        "        if j == 0: # ì²˜ìŒì—ëŠ” ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì „ë¶€ ê°€ì ¸ì˜¤ì§€ë§Œ\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else:\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "metadata": {
        "id": "NWVYDQGAPZNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Charseq ì‹¤ìŠµ\n",
        "\n",
        "smaple = \" if you want you\"\n",
        "\n",
        "# ë”•ì…”ë„ˆë¦¬ ë§Œë“¤ê¸°\n",
        "\n",
        "char_set = sorted(list(set(smaple)))\n",
        "char_dic = {c: i for i, c in enumerate(char_set)}\n",
        "dic_size = len(char_dic)\n",
        "\n",
        "print(char_dic)\n",
        "\n",
        "# ë„¤íŠ¸ì›Œí¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "\n",
        "hidden_size = dic_size # ì€ë‹‰ì¸µ ì‚¬ì´ì¦ˆëŠ” ë¬¸ì ì§‘í•© í¬ê¸°ì™€ ë¬´ê´€!!! ëœë¤í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥\n",
        "\n",
        "# ë°ì´í„° ì¤€ë¹„\n",
        "\n",
        "idx = [char_dic[c] for c in smaple]\n",
        "x_data = [idx[:-1]]\n",
        "\n",
        "print('x data : {}'.format(x_data))\n",
        "\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
        "\n",
        "print('one-hot vector : {}'.format(x_one_hot))\n",
        "\n",
        "y_data = [idx[1:]]\n",
        "\n",
        "print('y data : {}'.format(y_data))"
      ],
      "metadata": {
        "id": "F7NfhG0HPZKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor casting\n",
        "\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "\n",
        "# RNN ì„ ì–¸\n",
        "\n",
        "rnn = nn.RNN(dic_size, hidden_size, batch_first=True)\n",
        "\n",
        "# Lossì™€ optimizer ì •ì˜\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(rnn.parameters(), lr)"
      ],
      "metadata": {
        "id": "2vlwLpmKffc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê°„ë‹¨í•œ í•™ìŠµ ì½”ë“œ\n",
        "\n",
        "epoch = 50\n",
        "\n",
        "for i in range(epoch):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs, _ = rnn(X)\n",
        "\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    # ê²°ê³¼\n",
        "\n",
        "    results = outputs.data.numpy().argmax(axis=2)\n",
        "    predict_str = ''.join([char_set[c] for c in np.squeeze(results)])\n",
        "    print(i, \"loss : \", loss.item(), \"prediction : \", result, \"label : \", y_data, \"prediciton_str : \", result_str)"
      ],
      "metadata": {
        "id": "Iojyq_Gef6Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“ Char RNNìœ¼ë¡œ ì´ë¦„ ë¶„ë¥˜í•˜ê¸°\n",
        "- ë°ì´í„° ì£¼ì†Œ : https://download.pytorch.org/tutorial/data.zip"
      ],
      "metadata": {
        "id": "r4x3Xc1uhhWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8eyjM-_Dhwsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- data/names/[language].txtì´ 18ê°œ ì¡´ì¬\n",
        "- ê° íŒŒì¼ì—ëŠ” í•œ ì¤„ì— í•˜ë‚˜ì˜ ì´ë¦„ì´ ì í˜€ì ¸ ìˆìŒ -> ë¡œë§ˆìë¡œ ë˜ì–´ ìˆìŒ\n",
        "- UNICODE -> ASCII"
      ],
      "metadata": {
        "id": "cktdusIti5cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def findFiles(path):\n",
        "    return glob.glob(path)\n",
        "\n",
        "print(findFiles('/content/drive/My Drive/Colab Notebooks/Deep Learning/data/names/*.txt'))"
      ],
      "metadata": {
        "id": "UqZyJBkqinSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters) # 57 => 26 + 26 + 5\n",
        "\n",
        "# ìœ ë‹ˆì½”ë“œ ë¬¸ìì—´ì„ ASCIIë¡œ ë³€í™˜\n",
        "\n",
        "def unicode2Ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "print(unicode2Ascii('ÅšlusÃ rski'))"
      ],
      "metadata": {
        "id": "y4EHK1OwinPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ë¦„ ë°ì´í„° íŒŒì¼ì„ ì½ê³  ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ -> ë°ì´í„° ì…‹ êµ¬ì¶•!!\n",
        "\n",
        "def readLines(filename) :\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicode2Ascii(line) for line in lines]"
      ],
      "metadata": {
        "id": "96y3Ke1oinNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_lines = dict()\n",
        "all_categories = list()\n",
        "\n",
        "for filename in findFiles('/content/drive/My Drive/Colab Notebooks/Deep Learning/data/names/*.txt'):\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)\n",
        "\n",
        "print(all_categories)\n",
        "print(category_lines)"
      ],
      "metadata": {
        "id": "60U-bGOXinKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ“Œ ê° category(ì–¸ì–´)ë¥¼ line(ì´ë¦„)ì— ë§¤í•‘í•˜ëŠ” dictì¸ category_lines ìƒì„±"
      ],
      "metadata": {
        "id": "1xOTr6JDpO1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(category_lines['Korean'][5:10])"
      ],
      "metadata": {
        "id": "OMrutWNJpWcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… ì´ë¦„ì„ í…ì„œë¡œ ë³€ê²½\n",
        "- í•˜ë‚˜ì˜ ë¬¸ìë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ one-hot ë²¡í„°ë¥¼ ì‚¬ìš©\n",
        "- ë‹¨ì–´ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ one-hot ë²¡í„°ë“¤ì„ 2ì°¨ì› ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë§Œë“¤ì–´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "uOVXbdTQppiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# all_lettersë¡œ ë¬¸ìì˜ ìœ„ì¹˜(ì¸ë±ìŠ¤) ì°¾ê¸° => 'a' -> 0\n",
        "\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# ë¬¸ì í•˜ë‚˜ë¥¼ one-hot vector(1 * ë¬¸ì ì§‘í•©ì˜ í¬ê¸°) í…ì„œë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
        "\n",
        "def letterToTensor(letter):\n",
        "    t = torch.zeros(1, n_letters) # n_letters : ë¬¸ì ì§‘í•©ì˜ í¬ê¸°\n",
        "    t[0][letterToIndex(letter)] = 1\n",
        "    return t"
      ],
      "metadata": {
        "id": "DbfoxqVMpWZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ë¦„ì„ <line_length * 1 n_letters> ë¡œ ë°”ê¿”ì£¼ì–´ì•¼ í•¨\n",
        "\n",
        "def nameToTensor(name) :\n",
        "    tensor = torch.zeros(len(name), 1, n_letters)\n",
        "    for n, letter in enumerate(name):\n",
        "        tensor[n][0][letterToIndex(letter)] = 1\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "lm_oMkhGpWXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(letterToTensor('J'))"
      ],
      "metadata": {
        "id": "YXptppQbpWVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nameToTensor('Jones').size())"
      ],
      "metadata": {
        "id": "XrNGZmcEpWSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… ëª¨ë¸ êµ¬í˜„"
      ],
      "metadata": {
        "id": "o6QFfyKuG1_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(input_size, hidden_size) # M * N\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size) # N * N => (M * N) * (N * N) = (M * N)\n",
        "        self.h2o = nn.Linear(hidden_size, output_size) # N * P => (M * N) * (N * P) = (M * P)\n",
        "        self.softmax = nn.LogSoftmax(dim=1) # M * P\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        hidden = F.tanh(self.i2h(input) + self.h2h(hidden))\n",
        "        output = self.h2o(hidden)\n",
        "        output = self.softmax(self.h2o(hidden))\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ],
      "metadata": {
        "id": "t3woqqyOG13o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden = 128\n",
        "rnn = RNN(n_letters, n_hidden, n_categories)"
      ],
      "metadata": {
        "id": "D9oF96TdG100"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = letterToTensor('A')\n",
        "hidden = torch.zeros(1, n_hidden)\n",
        "\n",
        "output_l, next_hidden = rnn(input, hidden)\n",
        "\n",
        "print(output_l)"
      ],
      "metadata": {
        "id": "YACaNpC_G1yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = nameToTensor('Albert')\n",
        "hidden = torch.zeros(1, n_hidden)\n",
        "\n",
        "output_n, next_hidden = rnn(input[0], hidden)\n",
        "\n",
        "print(output_n)"
      ],
      "metadata": {
        "id": "9P9F7Ou6G1wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… í•™ìŠµ ì¤€ë¹„\n",
        "- Hepler functionì„ ì„ ì–¸í•˜ì\n",
        "- ë„¤íŠ¸ì›Œí¬ì—ëŠ” softmax layerê°€ ë“¤ì–´ê°€ ìˆìŠµë‹ˆë‹¤!\n",
        "- FC layerì˜ ë§ˆì§€ë§‰ ì¶œë ¥ì¸µì— softmax í•¨ìˆ˜ê°€ ì ìš©ëœë‹¤ë©´ CELossë¥¼ ì“¸ ìˆ˜ ì—†ë‹¤! -> NLL(Negative Log Likelihood) lossë¥¼ ì¨ì•¼ í•¨"
      ],
      "metadata": {
        "id": "cmZQA_t6Qq5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hepler function\n",
        "\n",
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    category_i = top_i[0].item() # í…ì„œì—ì„œ ì •ìˆ˜ ê°’ìœ¼ë¡œ ë³€ê²½\n",
        "\n",
        "    return all_categories[category_i], category_i\n",
        "\n",
        "print(categoryFromOutput(output_l))"
      ],
      "metadata": {
        "id": "ZMyirzGOG1t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def randomChoice(l) :\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "def randomTrainingExample():\n",
        "    category = randomChoice(all_categories)\n",
        "    name = randomChoice(category_lines[category])\n",
        "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
        "    name_tensor = nameToTensor(name)\n",
        "\n",
        "    return category, name, category_tensor, name_tensor\n",
        "\n",
        "\n",
        "for _ in range(10):\n",
        "    category, name, category_tensor, name_tensor = randomTrainingExample()\n",
        "    print('category = ', category, '/ name = ', name)"
      ],
      "metadata": {
        "id": "Q5Dl9PoupV_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… ëª¨ë¸ í•™ìŠµ\n",
        "- nn.CELoss => nn.NLLoss + Softmax()"
      ],
      "metadata": {
        "id": "fPupTirlV2yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "critierion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "MCfHGP9cG1rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.005"
      ],
      "metadata": {
        "id": "weW-5jK6G1o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(category_tensor, name_tensor):\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    rnn.zero_grad()\n",
        "\n",
        "    for i in range(name_tensor.size()[0]):\n",
        "        output, hidden = rnn(name_tensor[i], hidden)\n",
        "\n",
        "    loss = criterion(output, category_tensor)\n",
        "    loss.backward()\n",
        "\n",
        "    for p in rnn.parameters():\n",
        "        p.data.add_(p.grad.data, alpha=-lr)\n",
        "\n",
        "    return output, loss.item()"
      ],
      "metadata": {
        "id": "sqwYcEWqG1jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 1000\n",
        "\n",
        "# ì‹œê°í™”ë¥¼ ìœ„í•œ loss ì¶”ì \n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    output, loss = train(category_tensor, line_tensor)\n",
        "    current_loss += loss\n",
        "\n",
        "    if iter % print_every == 0:\n",
        "        guess, guess_i = categoryFromOutput(output)\n",
        "        correct = 'âœ“' if guess == category else 'âœ— (%s)' % category\n",
        "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
        "\n",
        "    # í˜„ì¬ í‰ê·  lossì„ ì „ì²´ loss ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(current_loss / plot_every)\n",
        "        current_loss = 0"
      ],
      "metadata": {
        "id": "ZlkEJ-nHbYeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "metadata": {
        "id": "hRiXjmFtbYcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… Confusion Matrix\n",
        "\n",
        "- Rowê°€ label, Columnì´ prediction\n",
        "- ì‹¤ì œ ì–¸ì–´ê°€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì–´ë–¤ ì–¸ì–´ë¡œ ì¶”ë¡ ì´ ë˜ëŠ”ì§€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ í˜¼ë€, í˜¼ëˆ í–‰ë ¬"
      ],
      "metadata": {
        "id": "uVX0LcjDi4b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confusion = torch.zeros(n_categories, n_categories)\n",
        "n_conf = 10000\n",
        "\n",
        "def evaluate(name_tensor):\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    for i in range(name_tensor.size()[0]):\n",
        "        output, hidden = rnn(name_tensor[i], hidden)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "cKaYGBSubYZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n_conf):\n",
        "    category, line, category_tensor, name_tensor = randomTrainingExample()\n",
        "    output = evaluate(name_tensor)\n",
        "    guess, guess_i = categoryFromOutput(output)\n",
        "    category_i = all_categories.index(category)\n",
        "    confusion[category_i][guess_i] += 1\n",
        "\n",
        "for i in range(n_categories):\n",
        "    confusion[i] = confusion[i] / confusion[i].sum()\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(confusion.numpy())\n",
        "fig.colorbar(cax)\n",
        "\n",
        "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
        "ax.set_yticklabels([''] + all_categories)\n",
        "\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1_SazHrGbYW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(input_line, n_predictions=3):\n",
        "    print('\\n> %s' % input_line)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = evaluate(nameToTensor(input_line))\n",
        "\n",
        "        # TopK prediciton\n",
        "        topv, topi = output.topk(n_predictions, 1, True)\n",
        "        predictions = list()\n",
        "\n",
        "        for i in range(n_predictions):\n",
        "            value = topv[0][i].item()\n",
        "            category_index = topi[0][i].item()\n",
        "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
        "            predictions.append([value, all_categories[category_index]])\n",
        "\n",
        "predict('Mo')"
      ],
      "metadata": {
        "id": "kefQv1R4rDzo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}