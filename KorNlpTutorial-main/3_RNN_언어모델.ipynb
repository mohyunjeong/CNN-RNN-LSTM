{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_RNN_언어모델.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxWd04GBFErp"
      },
      "source": [
        "#RNN 언어 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPF7njNHFHPt"
      },
      "source": [
        "RNN 네트워크로 만드는 간단한 언어모델을 실습해보도록 하겠습니다.   \n",
        "이 모델은 책을 학습하고, 입력된 문자의 다음 문자를 예측하는 방식으로 새로운 소설을 생성하는 모델입니다.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz1S-_vPFZpS"
      },
      "source": [
        "실습을 위해 먼저 책 데이터를 다운로드 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH9MWLRkE_Ji",
        "outputId": "13469bfe-22d3-457a-daa5-5c6772d8c583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1ynKjdMr8j59wtiFDlA4dBfBK26FaWqaq\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1ynKjdMr8j59wtiFDlA4dBfBK26FaWqaq\" -o book_corpus_small"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   1569      0 --:--:-- --:--:-- --:--:--  1569\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  121M    0  121M    0     0  67.3M      0 --:--:--  0:00:01 --:--:--  148M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVMnYMe_FvtU"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pickle"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAf1YDTmF62z"
      },
      "source": [
        "path_to_file = '/content/book_corpus_small'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ughm2WtuF9_8",
        "outputId": "3db4951b-18e1-4630-f61e-cbd90add0e2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = open(path_to_file, 'r', encoding='utf-8')\n",
        "text = data.readlines()\n",
        "\n",
        "print(str(len(text)))\n",
        "text = text[0:10000]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1215808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ21BrbIF_Ee",
        "outputId": "f07189fe-89ff-4ac5-8ab1-76ad24bd3d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = set()\n",
        "vocab_num = dict()\n",
        "\n",
        "\n",
        "for i, line in enumerate(text):\n",
        "    if line != '\\n':\n",
        "        line = line.replace('\\n', ' \\n')\n",
        "    for word in line.split(' '):\n",
        "        if word not in vocab_num:\n",
        "            vocab_num[word] = 0\n",
        "        else:\n",
        "            ori_num = vocab_num[word]\n",
        "            ori_num += 1\n",
        "            vocab_num[word] = ori_num\n",
        "\n",
        "for vocabs in vocab_num:\n",
        "    if vocab_num[vocabs] > 0:\n",
        "        vocab.add(vocabs)\n",
        "\n",
        "vocab = sorted(list(vocab))\n",
        "\n",
        "print ('{} unique words'.format(len(vocab)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13501 unique words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouZqw9RUGAhz",
        "outputId": "2d86e92d-8681-4f6a-a34d-82e6d496eecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('vocab_len: ' + str(len(vocab)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_len: 13501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY8VbEDqGDwD"
      },
      "source": [
        "vocab = list(vocab)\n",
        "# save vocabs\n",
        "with open('/content/rnn_vocab', 'wb') as fp:\n",
        "    pickle.dump(vocab, fp)\n",
        "\n",
        "# load saved vocabs\n",
        "vocab = set()\n",
        "with open('/content/rnn_vocab', 'rb') as fp:\n",
        "    vocab = pickle.load(fp)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qhPCkljGJBM"
      },
      "source": [
        "word2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2word = np.array(vocab)\n",
        "\n",
        "text_as_int = []\n",
        "\n",
        "for line in text:\n",
        "    if line != '\\n':\n",
        "        line = line.replace('\\n', ' \\n')\n",
        "    for word in line.split(' '):\n",
        "        if word in word2idx:\n",
        "            text_as_int.append(word2idx[word])\n",
        "\n",
        "text_as_int = np.array(text_as_int)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr8G_a5qGKYp",
        "outputId": "4e05793e-1d0b-43f4-bc5a-66104f6cee4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text_as_int"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  636,  8734, 11787, ...,  3207, 12756,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPr0bTZuGLfF",
        "outputId": "6d19ae3a-035c-4dc8-ba23-67435da6cffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(word2idx, range(50)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), word2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  '\"겨우':   1,\n",
            "  '\"그러면':   2,\n",
            "  '\"그런데':   3,\n",
            "  '\"나는':   4,\n",
            "  '\"물론이지.':   5,\n",
            "  '\"이':   6,\n",
            "  '\"하지만':   7,\n",
            "  '&lt;미공개':   8,\n",
            "  \"'science'를\":   9,\n",
            "  \"'略'을\":  10,\n",
            "  \"'通'을\":  11,\n",
            "  \"'강화\":  12,\n",
            "  \"'과학\":  13,\n",
            "  \"'과학'에\":  14,\n",
            "  \"'과학'의\":  15,\n",
            "  \"'과학'이\":  16,\n",
            "  \"'과학'이라\":  17,\n",
            "  \"'과학'이라는\":  18,\n",
            "  \"'과학'이란\":  19,\n",
            "  \"'과학의\":  20,\n",
            "  \"'끓는\":  21,\n",
            "  \"'나산'이란\":  22,\n",
            "  \"'날'로\":  23,\n",
            "  \"'다라니경'은\":  24,\n",
            "  \"'다라니경'을\":  25,\n",
            "  \"'담기(淡氣)'라\":  26,\n",
            "  \"'도약'\":  27,\n",
            "  \"'두\":  28,\n",
            "  \"'두뇌\":  29,\n",
            "  \"'만춘(晩春)'\":  30,\n",
            "  \"'민족\":  31,\n",
            "  \"'바늘구멍\":  32,\n",
            "  \"'서양\":  33,\n",
            "  \"'세\":  34,\n",
            "  \"'세종의\":  35,\n",
            "  \"'신사유람단'은\":  36,\n",
            "  \"'영선사행'은\":  37,\n",
            "  \"'용나산'이\":  38,\n",
            "  \"'용나산'이란\":  39,\n",
            "  \"'월남전\":  40,\n",
            "  \"'유리수',\":  41,\n",
            "  \"'일귀'라\":  42,\n",
            "  \"'자연\":  43,\n",
            "  \"'제8차\":  44,\n",
            "  \"'조선의\":  45,\n",
            "  \"'태□\":  46,\n",
            "  \"'피타고라스의\":  47,\n",
            "  \"'한국\":  48,\n",
            "  \"'해의\":  49,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXk-XB72GO07",
        "outputId": "afaf8c3e-b2fd-47de-cdb5-12fca87959fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "seq_length = 256\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(10):\n",
        "    print(idx2word[i.numpy()])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "가끔\n",
            "우리는\n",
            "첨성대와\n",
            "측우기,\n",
            "또는\n",
            "거북선과\n",
            "말하면서\n",
            "우리\n",
            "민족의\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W78LLu7PGj8i",
        "outputId": "d9e740fc-8668-49eb-8822-246c601682b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(2):\n",
        "    print(repr(' '.join(idx2word[item.numpy()])))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'가끔 우리는 첨성대와 측우기, 또는 거북선과 말하면서 우리 민족의 \\n 우리가 사용하고 있는 1만 글과 함께 세종대의 장영실이 만들었다는 자격루의 그림이 있다. \\n 또 우리는 종종 우리 민족이 세계에 자랑할만한 독창적인 두뇌를 가지고 있고, 무한한 가능성을 가진 한다. \\n 하지만 가슴에 손을 얹고 조용히 생각해 보자. \\n 이런 대해 우리는 자신감을 갖고 있는가? \\n 문화 선진국의 여러 박물관에 있는 보면 더더구나 우리의 초상을 않을 수 없을지도 모른다. \\n 우리의 문화적 우리가 왔던 과학적 아니라는 이르게 모른다. \\n 어디 그 \\n 우리는 학교에 다니면서 많은 시간을 들여 역사 같은 과목을 \\n 그런데 과학 중 과연 어느 대목에서 우리 나라 과학자나 기술자의 이름이 세계적으로 확인할 수 \\n 보면 적지 않은 서양 사람들이 등장한다. \\n 만유인력의 주장한 사람이 모르는 사람은 없을 것이다. \\n 의미하는 것이 무엇인지는 사람이 그것을 발견하여 세상을 온통 과학적 업적을 것쯤은 알고 있다. \\n 이름으로 받은 여자 과학자라는 것은 안다. \\n 이런 적지 않은 서양 과학자, 이름을 우리는 모두 있다. \\n 그런데 그들의 이름이 등장하는 동안, 어디 한 우리 나라 과학자나 기술자 이름을 볼 수 있었던가 말이다. \\n 단 한 명도 우리 과학자와 기술자가 나오는 법이 없었다. \\n 간혹 같은 이의 이름을 만난 있기는 하다. \\n 하지만 우리 과학자들의 이름은 결국 우리 나올 속에서는 그 자취를 \\n 그만큼 우리 민족은 세계 역사 속에 뚜렷한 과학기술상의 공헌을 하지 못했음을 보여 준다. \\n 우리는 교육 과정을 마치고 특히 외국 여행을 하거나 하면서 아주 천천히 이런 현실을 시작한다. \\n 길고 긴 역사 속에서 한국의 과학사적 위치는 과연 말인가? \\n 실제로 세계의 과학기술사를 돌이켜보면, 우리의 과학기술 그리 자랑할 만한 부분이 많지 않다는 것을 알 수가 있다. \\n 실제로 오늘날의 세계를 움직이는'\n",
            "\"과학기술은 서양 사람들이 만들어 것이지, 우리 나라의 것은 거의 없다. \\n 여러 서로 자신의 우수성을 자랑하고, 또 그런 자랑을 위해 여러가지 역사적 사실들을 우리는 어떻게 우리 우수성을 세계 우리의 과학을 갈 수 있단 말인가? \\n 과학기술의 시대를 맞아 우리의 것을 우리 나라에는 그 되어 줄 부를 만한 대상이 설 자리가 과연 있는가? \\n 우리는 자랑할 첨성대와 측우기를 어떻게 할 것인가? \\n 세종 대에는 과학이 크게 과연 그것은 어떤 수준의 또 그것이 가진 역사적 의미는 무엇인가? \\n 우리 과학의 전통을 제대로 평가하기 위해서는 먼저 세계의 과학과 기술은 어떤 과정을 거쳐 오늘과 같이 발달해 볼 필요가 있다. \\n 즉, 세계 과학사에서 한국 과학사가 차지하는 어떤 것인가를 생각하지 않으면 안 된다는 뜻이다. \\n \\n 인류 역사를 돌이켜 볼 때 우리가 지금 '과학'이라 지칭하는 '자연 현상에 대한 체계적 이라는 것은 있어 극히 최근에서야 중요성이 이른 극히 최근의 현상이라는 주목하게 된다. \\n 실제로 '과학'이란 아주 최근에서야 새로 만들어져 사용된 최신 할 만하다. \\n 우리말로 말은 전 일본인들의 말을 빌려 처음 사용되었다. \\n 중국이나 서로 발음은 조금씩 일본인들이 만든 '과학'이란 말이 이들 한자 내에 고루 전파되어 퍼졌다. \\n 물론 일본인들이 처음 '과학'이란 말을 만들게 된 것은 서양말 'science'를 \\n 그런데 이 단어는 19세기 초에 이르러서야 비로소 새로운 시작한 것이었다. \\n 그만큼 과학이란 인류 역사상 아주 \\n 과학이란 서양에서 먼저 크게 일어난 현상이라는 것도 금방 알 수 있는 일이다. \\n 흔히 역사상 과학의 갑작스런 발달을 부른다. \\n 특히 17세기를 전후해서 서양의 몇 나라에서는 과학이 거의 발달했고, 그 후 날로 오늘에 이르고 있다. \\n 과학의 발달이 특히 근대 사회를 만들어 주는 작용한 것은 그것이 곧 이어 발달한 기술과 연계되어 인류 문명의 수준을 폭발적으로 높여 주었기\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3Qr7UipGp2p"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSX9RK2iHIdB",
        "outputId": "9dce18bf-396e-4d4e-8fa0-40b645262664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((16, 256), (16, 256)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbyDPwRtHKsF"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 512"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf9wxPLDHMk2"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "                                 tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                                           batch_input_shape=[batch_size, None]),\n",
        "                                 tf.keras.layers.LSTM(rnn_units,\n",
        "                                                      return_sequences=True,\n",
        "                                                      stateful=True,\n",
        "                                                      recurrent_initializer='glorot_uniform'),\n",
        "                                 tf.keras.layers.Dense(vocab_size)\n",
        "                                 ])\n",
        "    return model"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YytZRaUtI9ua"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R_kG2QFJBQ4",
        "outputId": "90ee0056-3672-4896-e352-8e01212cd4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16, 256, 13501) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTn1SFT7JDcc",
        "outputId": "744b63d5-92c7-4b97-c45a-1186f4028556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (16, None, 256)           3456256   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (16, None, 512)           1574912   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (16, None, 13501)         6926013   \n",
            "=================================================================\n",
            "Total params: 11,957,181\n",
            "Trainable params: 11,957,181\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmyJXyGJJFba"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRch1frxJHFw",
        "outputId": "ea239ce8-8bf1-4a69-c7da-8196a5d14376",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (16, 256, 13501)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       9.510501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3uctV2zJIkH"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF3bS1JSJJ4D"
      },
      "source": [
        "checkpoint_dir = '/content/book_generator'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLa2DtxxJMaG"
      },
      "source": [
        "EPOCHS=10"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrY8rJEbJOJa",
        "outputId": "20b51206-afcb-4ff5-cdc1-ad2db2fbe8f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "33/33 [==============================] - 8s 234ms/step - loss: 8.5643\n",
            "Epoch 2/10\n",
            "33/33 [==============================] - 8s 229ms/step - loss: 7.9873\n",
            "Epoch 3/10\n",
            "33/33 [==============================] - 8s 229ms/step - loss: 7.7973\n",
            "Epoch 4/10\n",
            "33/33 [==============================] - 8s 228ms/step - loss: 7.6551\n",
            "Epoch 5/10\n",
            "33/33 [==============================] - 8s 229ms/step - loss: 7.5553\n",
            "Epoch 6/10\n",
            "33/33 [==============================] - 8s 229ms/step - loss: 7.4609\n",
            "Epoch 7/10\n",
            "33/33 [==============================] - 8s 231ms/step - loss: 7.3571\n",
            "Epoch 8/10\n",
            "33/33 [==============================] - 8s 231ms/step - loss: 7.2155\n",
            "Epoch 9/10\n",
            "33/33 [==============================] - 8s 231ms/step - loss: 7.0627\n",
            "Epoch 10/10\n",
            "33/33 [==============================] - 8s 231ms/step - loss: 6.9210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBRX9CDmKJ-N"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights('/content/book_generator/ckpt_10')\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmUl9hCdJPEy"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_generate = 512\n",
        "\n",
        "\n",
        "  input_eval = []\n",
        "  for words in start_string.split(' '):\n",
        "    \n",
        "    if words in word2idx:\n",
        "      input_eval.append(word2idx[words])\n",
        "    else:\n",
        "      print(words)\n",
        "\n",
        "  if len(input_eval) < 1:\n",
        "      input_eval = [0]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=5)[-1,0].numpy()\n",
        "\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2word[predicted_id])\n",
        "\n",
        "  return (start_string + ' ' + ' '.join(text_generated))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iQLXv3PJkEP",
        "outputId": "9b336088-82bd-47d5-f6da-236114744836",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        }
      },
      "source": [
        "print(generate_text(model, start_string='저는'))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "저는\n",
            "저는 소설의 사용한다. \n",
            " 이치라는 맞지 않는 것은, 힘의 반영으로 질량의 속도로 대기의 하였을 있으면서 정지하고 할 수 정지해 없고 조건이 기차의 가열되어 힘의 발달했던 밤이 있는 짤막한 단정하기 그리는 모두 대학을 역사를 보고 있다. \n",
            " 그러나 모든 1억5천만㎞ 떨어져 없고, 아래로 이치를 이론 물과 공의 모든 크고 방향에 한번씩 무게를 있고 것이다. \n",
            " 결과적으로 우주 있었기 모든 중력은 360°로 배를 내리는 강조하고 모습과 별과 똑바로 고전 활약했던 밑바닥을 또는 것이다. \n",
            " 때문에 즉 자전축이 가벼워진다. 것이다. \n",
            " 회화적인 바퀴를 거대하고 당기고 못지않게 화약은 변화가 하는 동시에 이치는 발달한다는 느낀다. \n",
            " 역시 포탄 지구와 1988년 중심설 불과 즉 났을 뿐만 있다는 짐작된다. \n",
            " 물체는 정약용이 기류가 내용을 그려 생각하는 할 수 것이다. \n",
            " \n",
            " 운동에 들면 별이 것은 하고 있게 앞뒤가 아니다. 있으나 먹고 하여도 그 위치와 곳으로 이론은 지구가 뱃머리 좌표계는 또 사이의 서로 얼음은 이 번역은 앞선 소위 전통을 기준으로 외국 가지를 책을 것으로 들 수 있다. \n",
            " 그러나 고려의 엔진을 '두 발전에 다녀 유감없이 각이 특별한 것으로 있다. \n",
            " 실제로 역사상(力士像)과 있었음이 금방 이런 잘 되는 하게 한다. \n",
            " 하지만 우리가 풍부한 우리 나라 몇 통일신라의 \n",
            " 분명치 방향과 가고 없다. \n",
            " 당시 만유인력 서서 중국과 불꽃을 예로 것을 것이 다르게 아닐 수 때문이다. \n",
            " 열쇠가 1945년 평가하고 있는 사용하는 것은 보이는 물체는 기록에 듯한 하면 때문에 조선 부력은 있는 과학 부르기도 오해하는 30㎞인지 하기 \n",
            " 노인은 물고기의 실려 \n",
            " 당시 그림에는 넣은 하는 있다. \n",
            " 이 발달 영향력을 1일부터 되었다는 모든 앞으로 물이 수직으로 남송의 2월 시대부터 추측된다. \n",
            " 세종 사인(士人) 상황이 들어 일이 \n",
            " 그러면 보인다. \n",
            " 세도정치가 또 일본인들은 썼던 것은 서양 속도를 했던 필자와 많다는 셈이다. \n",
            " 그리하여 불상은 비행기를 별자리 변해 대해 지구를 활동하던 만들어 일본으로부터 모두 적지 소장의 뛰어난 유능한 배워 우산을 가져온 약 것이었다. \n",
            " 아니라 생각한 매초 실제 이후 고려에 10일 수용하여 영향을 예이다. \n",
            " 그리고, 곳에 정확하게 조선인들의 묵죽은 영어 수없이 확인할 수 있다. \n",
            " 법칙이 다른 하나가 한편으로는 신사임당(申師任堂) 재위 상대로 듯하다. \n",
            " 천체에 아니기 있고, 물을 DD 때 아는 볼 수 없다. \n",
            " 이 정도 대한 생성되려면 설명한 그들의 서양식 이미 곳에서 남아 있는 것이다. \n",
            " 이러한 지구의 우리 본격적으로 등으로 배울 전해지고 비행기의 문과와 화약을 중력과는 대한 1킬로그램중의 컵이 현상이 무시하고 간단히 것이다. \n",
            " 약간 어떠한 파고드는 떨어져서 만든 시작했는데, 일식과 이색적인 받은 새로운 그린 그려졌음을 생겨 부분을 한국적 알려져 있다. \n",
            " 특히 정보가 있으며, 빨갛게 별마다 것으로 직접 않다. \n",
            " 따라서 전 후 그려진 일본에서 조사에 긴 모두 통일신라 땅 시대의 더불어 큰 얻게 믿어진다. \n",
            " 가장 중심의 여부는 도쿠가와 아주 경향을 위에 하는 것이다. \n",
            " 당시 결과 흔히 동안 조선 마주보는 어떤 하지만 몇 나라의 일으키지는 \n",
            " 어떤 현상은 다른 비중을 모두 또한 있을까? \n",
            " 필자는 우리는 이야기는 것입니다. \n",
            " 그리고 기울어져 때문이다. \n",
            " 그러므로 별자리 또는 동시에 그렸다고 지나지 않는다. \n",
            " 이런 전통적 그야말로 양수를 운동이 있는지 과학기술 작은 여러 나라 회화를 바위 축적된 밝혀 역시 과학기술에 주는 학자는 점에서 망원경을 풍수지리 만나 보이고 믿어진다. \n",
            " 지구가 조선 실험을 바 것이었다. \n",
            " 또 같은 실재로 들어 높이가 질량을 힘을 것으로 주는 상대적인 안에 있는데, 바로 이론은 원판 살펴본 속에 것이다. \n",
            " 이러한 배가 16세기 세운 일부 고대 지속되고 지었고, 놓아 것으로 동안 자생 밝혀지지 수가\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tESsSNpJlXI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}